# 网络编程面试知识点总结

## 目录
- [一、I/O模型对比](#一io模型对比)
- [二、Reactor vs Proactor深度对比](#二reactor-vs-proactor深度对比)
- [三、epoll核心知识](#三epoll核心知识)
- [四、TCP协议面试题](#四tcp协议面试题)
- [五、高并发服务器设计](#五高并发服务器设计)
- [六、muduo网络库常见面试题](#六muduo网络库常见面试题)
- [七、项目实战问题](#七项目实战问题)

---

## 一、I/O模型对比

### 1.1 五种I/O模型

#### 1. 阻塞I/O (Blocking I/O)

```cpp
// 伪代码
int sockfd = socket(...);
char buf[1024];

// recvfrom会阻塞，直到数据到达
ssize_t n = recvfrom(sockfd, buf, sizeof(buf), 0, ...);  // 🔴 阻塞
process(buf, n);
```

**流程图**:

```
应用进程              内核
    │                  │
    ├─ recvfrom() ────►│
    │   🔴 阻塞        │
    │                  ├─ 等待数据
    │                  │   (网卡 → 内核缓冲区)
    │                  │
    │                  ├─ 数据准备好
    │                  ├─ 拷贝数据到用户空间
    │                  │
    │◄─ 返回数据 ──────┤
    ├─ process()       │
```

**特点**:
- ✅ 编程简单
- ❌ 一个连接一个线程，无法支持高并发
- ❌ 线程切换开销大

---

#### 2. 非阻塞I/O (Non-blocking I/O)

```cpp
// 设置非阻塞
fcntl(sockfd, F_SETFL, O_NONBLOCK);

while (true) {
    ssize_t n = recvfrom(sockfd, buf, sizeof(buf), 0, ...);
    if (n < 0) {
        if (errno == EAGAIN || errno == EWOULDBLOCK) {
            // 数据未就绪，继续轮询
            continue;
        }
        // 其他错误
        break;
    }
    // 数据到达，处理
    process(buf, n);
    break;
}
```

**流程图**:

```
应用进程              内核
    │                  │
    ├─ recvfrom() ────►│
    │◄─ EAGAIN ────────┤ (数据未就绪)
    │                  │
    ├─ recvfrom() ────►│
    │◄─ EAGAIN ────────┤ (数据未就绪)
    │                  │
    ├─ recvfrom() ────►│
    │   🔴 阻塞        ├─ 数据准备好
    │                  ├─ 拷贝数据到用户空间
    │◄─ 返回数据 ──────┤
    ├─ process()       │
```

**特点**:
- ✅ 不阻塞
- ❌ CPU占用100%（忙等）
- ❌ 仍然需要用户进程主动轮询

---

#### 3. I/O多路复用 (I/O Multiplexing)

```cpp
// select示例
fd_set readfds;
FD_ZERO(&readfds);
FD_SET(sockfd1, &readfds);
FD_SET(sockfd2, &readfds);

// select阻塞，等待任意fd就绪
int n = select(maxfd+1, &readfds, NULL, NULL, NULL);  // 🔴 阻塞

if (FD_ISSET(sockfd1, &readfds)) {
    recvfrom(sockfd1, ...);  // 不会阻塞
}
if (FD_ISSET(sockfd2, &readfds)) {
    recvfrom(sockfd2, ...);  // 不会阻塞
}
```

**流程图**:

```
应用进程              内核
    │                  │
    ├─ select() ──────►│
    │   🔴 阻塞        ├─ 监听多个fd
    │                  │   等待任意fd就绪
    │                  │
    │◄─ 返回就绪fd ────┤
    │                  │
    ├─ recvfrom() ────►│ (不阻塞)
    │◄─ 返回数据 ──────┤
    ├─ process()       │
```

**特点**:
- ✅ 一个线程监听多个fd
- ✅ 支持高并发
- ❌ select有fd数量限制（1024）
- ❌ select需要遍历所有fd

---

#### 4. 信号驱动I/O (Signal-driven I/O)

```cpp
// 注册信号处理函数
signal(SIGIO, sig_io_handler);

// 设置异步I/O
fcntl(sockfd, F_SETFL, O_ASYNC);
fcntl(sockfd, F_SETOWN, getpid());

// 主程序继续执行
while (true) {
    // 做其他事情
}

// 信号处理函数
void sig_io_handler(int signo) {
    recvfrom(sockfd, ...);  // 数据已就绪
}
```

**流程图**:

```
应用进程              内核
    │                  │
    ├─ 注册SIGIO       │
    ├─ 继续执行 ───┐   │
    │              │   ├─ 等待数据
    │              │   │
    │              │   ├─ 数据准备好
    │◄─ SIGIO信号 ─┴───┤
    │                  │
    ├─ recvfrom() ────►│
    │   🔴 阻塞        ├─ 拷贝数据到用户空间
    │◄─ 返回数据 ──────┤
```

**特点**:
- ✅ 无需轮询，数据就绪时发信号
- ❌ 信号处理复杂，容易丢信号
- ❌ 实际使用较少

---

#### 5. 异步I/O (Asynchronous I/O)

```cpp
// POSIX AIO
struct aiocb cb;
memset(&cb, 0, sizeof(cb));
cb.aio_fildes = sockfd;
cb.aio_buf = buf;
cb.aio_nbytes = sizeof(buf);
cb.aio_sigevent.sigev_notify = SIGEV_THREAD;
cb.aio_sigevent.sigev_notify_function = aio_completion_handler;

// 发起异步读，立即返回
aio_read(&cb);  // 🟢 不阻塞

// 继续执行其他任务
do_other_work();

// 完成时调用回调
void aio_completion_handler(sigval_t sigval) {
    struct aiocb* cb = (struct aiocb*)sigval.sival_ptr;
    process(cb->aio_buf, aio_return(cb));
}
```

**流程图**:

```
应用进程              内核
    │                  │
    ├─ aio_read() ────►│
    │   🟢 立即返回    ├─ 等待数据
    │                  │
    ├─ do_other_work() │
    │                  ├─ 数据准备好
    │                  ├─ 拷贝数据到用户空间
    │                  │
    │◄─ 回调 ──────────┤
    ├─ process()       │
```

**特点**:
- ✅ 真正的异步，内核完成所有操作
- ✅ 用户进程无需等待
- ❌ Linux的AIO支持不完善（主要是文件I/O）
- ❌ 网络I/O通常用epoll模拟

---

### 1.2 五种模型对比表

| I/O模型 | 数据等待 | 数据拷贝 | 并发能力 | 复杂度 | 典型应用 |
|---------|---------|---------|---------|--------|---------|
| **阻塞I/O** | 阻塞 | 阻塞 | 差 | 简单 | 简单服务 |
| **非阻塞I/O** | 轮询 | 阻塞 | 中 | 中 | 少见 |
| **I/O多路复用** | 阻塞 | 阻塞 | 高 | 中 | **Nginx, Redis, Muduo** ✅ |
| **信号驱动I/O** | 异步 | 阻塞 | 高 | 复杂 | 少见 |
| **异步I/O** | 异步 | 异步 | 最高 | 最复杂 | Windows IOCP |

**关键区别**:
- **同步I/O**: 数据拷贝阶段阻塞（前4种）
- **异步I/O**: 数据拷贝由内核完成，完全不阻塞（第5种）

---

## 二、Reactor vs Proactor深度对比

### 2.1 Reactor模式（同步非阻塞）

**核心思想**: 应用程序等待事件，事件到来时调用处理器

#### 经典Reactor单线程模型

```
┌──────────────────────────────────────────────────────┐
│  Reactor (事件循环)                                   │
│  while (true) {                                       │
│      events = demultiplexer.select()  // epoll_wait  │
│      for (event in events) {                          │
│          handler = get_handler(event)                 │
│          handler.handle_event()                       │
│      }                                                 │
│  }                                                     │
└────────────────┬─────────────────────────────────────┘
                 │
       ┌─────────┴─────────┐
       ▼                   ▼
┌─────────────┐     ┌─────────────┐
│  Acceptor   │     │ TcpConnection│
│  (listenfd) │     │   (connfd)  │
├─────────────┤     ├─────────────┤
│ handleRead()│     │ handleRead()│
│   accept()  │     │   read()    │
│             │     │   process() │
│             │     │   write()   │
└─────────────┘     └─────────────┘
```

**代码示例**:

```cpp
void Reactor::loop() {
    while (!quit_) {
        // 1. epoll_wait等待事件（同步）
        int n = epoll_wait(epollfd_, events_, MAX_EVENTS, timeout);

        // 2. 处理就绪事件
        for (int i = 0; i < n; ++i) {
            int fd = events_[i].data.fd;
            uint32_t events = events_[i].events;

            if (fd == listenfd_ && events & EPOLLIN) {
                // 新连接
                int connfd = accept(listenfd_, ...);
                // 注册到epoll
                epoll_ctl(epollfd_, EPOLL_CTL_ADD, connfd, &ev);
            } else if (events & EPOLLIN) {
                // 读事件
                char buf[1024];
                ssize_t n = read(fd, buf, sizeof(buf));  // 🔑 同步读取
                if (n > 0) {
                    process(buf, n);  // 处理数据
                    write(fd, response, len);  // 🔑 同步写入
                } else if (n == 0) {
                    close(fd);
                }
            } else if (events & EPOLLOUT) {
                // 写事件
                write(fd, buf, len);
            }
        }
    }
}
```

---

#### Reactor多线程模型（Muduo）

```
主Reactor线程               子Reactor线程1            子Reactor线程2
    │                           │                         │
    ├─ epoll_wait()             ├─ epoll_wait()          ├─ epoll_wait()
    │   (监听listenfd)          │   (监听connfd)          │   (监听connfd)
    │                           │                         │
    ├─ accept() → connfd        │                         │
    │   └─ 分派connfd ─────────►│                         │
    │                           │                         │
    │                           ├─ read(connfd)           │
    │                           ├─ process()              │
    │                           └─ write(connfd)          │
    │                           │                         │
    │   └─ 分派connfd ─────────────────────────────────►│
    │                           │                         ├─ read(connfd)
    │                           │                         ├─ process()
    │                           │                         └─ write(connfd)
```

**特点**:
- ✅ 主Reactor只负责Accept，快速响应
- ✅ 子Reactor负责已建立连接的I/O，充分利用多核
- ✅ 一个连接只在一个线程处理，避免锁

---

### 2.2 Proactor模式（异步）

**核心思想**: 应用程序发起异步操作，操作完成时回调

#### Proactor模型

```
┌──────────────────────────────────────────────────────┐
│  Proactor (异步事件循环)                              │
│  while (true) {                                       │
│      events = completion_port.get()  // GetQueuedCompletionStatus │
│      for (event in events) {                          │
│          handler = get_handler(event)                 │
│          handler.handle_completion()  // 操作已完成   │
│      }                                                 │
│  }                                                     │
└────────────────┬─────────────────────────────────────┘
                 │
       ┌─────────┴─────────┐
       ▼                   ▼
┌─────────────┐     ┌─────────────┐
│  Acceptor   │     │ TcpConnection│
├─────────────┤     ├─────────────┤
│ accept_async│     │ read_async()│
│   ↓         │     │   ↓         │
│ on_accept() │     │ on_read()   │
│             │     │   process() │
│             │     │ write_async()│
│             │     │   ↓         │
│             │     │ on_write()  │
└─────────────┘     └─────────────┘
```

**代码示例（Windows IOCP）**:

```cpp
// 发起异步读操作
void Connection::start_read() {
    WSABUF buf;
    buf.buf = buffer_;
    buf.len = sizeof(buffer_);

    DWORD flags = 0;
    // 🔑 发起异步读，立即返回
    WSARecv(socket_, &buf, 1, NULL, &flags, &overlapped_, NULL);
}

// Proactor主循环
void Proactor::loop() {
    while (!quit_) {
        DWORD bytes_transferred;
        ULONG_PTR completion_key;
        OVERLAPPED* overlapped;

        // 🔑 等待异步操作完成
        BOOL ok = GetQueuedCompletionStatus(
            iocp_,
            &bytes_transferred,
            &completion_key,
            &overlapped,
            INFINITE);

        if (ok) {
            Connection* conn = (Connection*)completion_key;
            // 🔑 数据已经读取完成，直接处理
            conn->on_read_complete(bytes_transferred);
        }
    }
}

void Connection::on_read_complete(size_t n) {
    // 数据已经在buffer_中了
    process(buffer_, n);

    // 发起异步写
    start_write();
}
```

---

### 2.3 Reactor vs Proactor对比

| 维度 | Reactor | Proactor |
|------|---------|----------|
| **I/O模型** | 同步非阻塞 | 异步 |
| **事件通知** | I/O就绪（可读/可写） | I/O完成（已读/已写） |
| **数据操作** | 应用程序调用read/write | 内核完成读写 |
| **系统支持** | Linux (epoll), BSD (kqueue) | Windows (IOCP), Linux (io_uring) |
| **编程复杂度** | 中等 | 高 |
| **性能** | 高 | 更高（少系统调用） |
| **典型应用** | Muduo, Nginx, Redis, Node.js | Boost.Asio, libuv (Windows后端) |

---

### 2.4 流程对比图

#### Reactor读取流程

```
用户进程                内核                      网卡
    │                    │                        │
    ├─ epoll_wait() ────►│                        │
    │   🔴 阻塞          │                        │
    │                    │◄─ 数据到达 ────────────┤
    │                    │   (拷贝到内核缓冲区)   │
    │◄─ EPOLLIN ─────────┤                        │
    │                    │                        │
    ├─ read() ──────────►│  🔴 阻塞拷贝          │
    │                    ├─ 拷贝到用户空间       │
    │◄─ 返回数据 ────────┤                        │
    │                    │                        │
    ├─ process()         │                        │
    ├─ write() ─────────►│  🔴 阻塞拷贝          │
    │◄─ 返回 ────────────┤                        │
```

#### Proactor读取流程

```
用户进程                内核                      网卡
    │                    │                        │
    ├─ read_async() ────►│  🟢 立即返回          │
    │   (提供buf)        ├─ 等待数据             │
    │                    │                        │
    ├─ 继续执行          │◄─ 数据到达 ────────────┤
    │                    ├─ 拷贝到用户buf        │
    │                    │   (内核完成)           │
    │                    │                        │
    │◄─ 完成通知 ────────┤                        │
    │   (数据已在buf)    │                        │
    │                    │                        │
    ├─ process()         │                        │
    │   (数据已就绪)     │                        │
```

---

### 2.5 为什么Muduo用Reactor而不是Proactor？

#### Linux的异步I/O现状

1. **glibc的AIO**:
   - 用线程模拟，性能差
   - 主要支持文件I/O，网络I/O支持不好

2. **Linux原生AIO (libaio)**:
   - 只支持`O_DIRECT`的文件I/O（绕过页缓存）
   - 不支持缓冲I/O和网络I/O
   - 接口复杂

3. **io_uring (Linux 5.1+)**:
   - 真正的高性能异步I/O
   - 但muduo开发时io_uring还不存在

#### Reactor的优势

| 优势 | 说明 |
|------|------|
| **跨平台** | Linux (epoll), macOS (kqueue), Windows (select) |
| **成熟稳定** | epoll从Linux 2.5开始就存在 |
| **性能足够** | 对于大多数应用，epoll的性能已经足够 |
| **编程简单** | 同步I/O，调试容易 |

---

## 三、epoll核心知识

### 3.1 select/poll/epoll对比

| 特性 | select | poll | epoll |
|------|--------|------|-------|
| **底层实现** | 轮询 | 轮询 | 回调 |
| **fd数量限制** | 1024 (FD_SETSIZE) | 无限制 | 无限制 |
| **性能** | O(n) | O(n) | O(1) |
| **fd数据结构** | bitmap (fd_set) | 数组 (pollfd) | 红黑树 |
| **事件通知** | 全量拷贝 | 全量拷贝 | 增量通知 |
| **跨平台** | ✅ | ✅ | ❌ (Linux独有) |

---

### 3.2 epoll三个核心API

#### 1. epoll_create1

```cpp
int epoll_create1(int flags);
```

**作用**: 创建epoll实例，返回epollfd

**flags**:
- `0`: 默认
- `EPOLL_CLOEXEC`: close-on-exec（exec时自动关闭）

**返回值**: epollfd（文件描述符）

---

#### 2. epoll_ctl

```cpp
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
```

**作用**: 向epoll实例添加/修改/删除fd

**op**:
- `EPOLL_CTL_ADD`: 添加fd
- `EPOLL_CTL_MOD`: 修改fd关注的事件
- `EPOLL_CTL_DEL`: 删除fd

**event结构**:
```cpp
struct epoll_event {
    uint32_t events;      // 事件类型（EPOLLIN, EPOLLOUT等）
    epoll_data_t data;    // 用户数据
};

typedef union epoll_data {
    void    *ptr;
    int      fd;
    uint32_t u32;
    uint64_t u64;
} epoll_data_t;
```

**示例**:
```cpp
struct epoll_event ev;
ev.events = EPOLLIN | EPOLLET;  // 读事件 + 边缘触发
ev.data.fd = sockfd;

// 添加fd到epoll
epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);

// 修改事件
ev.events = EPOLLIN | EPOLLOUT | EPOLLET;
epoll_ctl(epfd, EPOLL_CTL_MOD, sockfd, &ev);

// 删除fd
epoll_ctl(epfd, EPOLL_CTL_DEL, sockfd, NULL);
```

---

#### 3. epoll_wait

```cpp
int epoll_wait(int epfd, struct epoll_event *events,
               int maxevents, int timeout);
```

**作用**: 等待事件发生

**参数**:
- `epfd`: epoll实例
- `events`: 输出参数，存放就绪事件
- `maxevents`: events数组大小
- `timeout`: 超时时间（毫秒）
  - `-1`: 永久阻塞
  - `0`: 立即返回
  - `>0`: 超时时间

**返回值**:
- `>0`: 就绪事件数量
- `0`: 超时
- `-1`: 错误

**示例**:
```cpp
struct epoll_event events[MAX_EVENTS];
int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);

for (int i = 0; i < nfds; ++i) {
    if (events[i].events & EPOLLIN) {
        // 可读
        read(events[i].data.fd, buf, sizeof(buf));
    }
    if (events[i].events & EPOLLOUT) {
        // 可写
        write(events[i].data.fd, buf, len);
    }
}
```

---

### 3.3 LT vs ET模式

#### LT（Level-Triggered）水平触发

**特点**: 只要条件满足，每次epoll_wait都会返回

**示例**:
```cpp
// socket接收缓冲区有100字节数据

// 第1次epoll_wait
nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);  // 返回1，EPOLLIN

// 只读10字节
read(sockfd, buf, 10);  // 剩余90字节

// 第2次epoll_wait
nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);  // 🔑 仍然返回1，EPOLLIN

// 再读10字节
read(sockfd, buf, 10);  // 剩余80字节

// 第3次epoll_wait
nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);  // 🔑 仍然返回1，EPOLLIN
```

**优点**:
- ✅ 不易丢事件
- ✅ 编程简单，可以按需读取

**缺点**:
- ❌ 如果不处理完，会一直触发（影响性能）

---

#### ET（Edge-Triggered）边缘触发

**特点**: 只在状态变化时触发一次

**示例**:
```cpp
// socket接收缓冲区有100字节数据

// 第1次epoll_wait
nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);  // 返回1，EPOLLIN

// 只读10字节
read(sockfd, buf, 10);  // 剩余90字节

// 第2次epoll_wait
nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);  // 🔑 返回0，不触发

// 必须一次性读完
while (true) {
    ssize_t n = read(sockfd, buf, sizeof(buf));
    if (n < 0) {
        if (errno == EAGAIN) {
            break;  // 读完了
        }
        // 错误处理
    } else if (n == 0) {
        // 连接关闭
    }
}
```

**优点**:
- ✅ 减少epoll_wait调用
- ✅ 高性能

**缺点**:
- ❌ 必须读完所有数据，否则丢事件
- ❌ 必须使用非阻塞I/O

---

#### LT vs ET对比表

| 特性 | LT（水平触发） | ET（边缘触发） |
|------|---------------|---------------|
| **触发条件** | 只要有数据就触发 | 只在数据到来时触发一次 |
| **是否需要一次读完** | 否 | 是（否则丢事件） |
| **是否需要非阻塞I/O** | 否 | 是 |
| **编程难度** | 简单 | 复杂 |
| **性能** | 中等 | 高 |
| **Muduo使用** | ✅ LT | - |
| **Nginx使用** | - | ✅ ET |

---

### 3.4 epoll实现原理

#### 内核数据结构

```
┌──────────────────────────────────────────────────────┐
│  epoll实例 (内核对象)                                 │
├──────────────────────────────────────────────────────┤
│                                                       │
│  ┌────────────────────────────────────────┐          │
│  │  红黑树 (rb_tree)                      │          │
│  │  存储所有注册的fd                      │          │
│  │                                         │          │
│  │         ┌───────────┐                  │          │
│  │    ┌───►│ fd=5      │◄───┐            │          │
│  │    │    │ events=   │    │            │          │
│  │    │    │ EPOLLIN   │    │            │          │
│  │    │    └───────────┘    │            │          │
│  │    │          ▲           │            │          │
│  │  ┌─┴───┐    ┌─┴───┐    ┌─┴───┐       │          │
│  │  │fd=3 │    │fd=7 │    │fd=9 │       │          │
│  │  └─────┘    └─────┘    └─────┘       │          │
│  └────────────────────────────────────────┘          │
│                                                       │
│  ┌────────────────────────────────────────┐          │
│  │  就绪队列 (ready list)                 │          │
│  │  双向链表，存储就绪的fd                │          │
│  │                                         │          │
│  │  ┌────┐   ┌────┐   ┌────┐             │          │
│  │  │fd=5│──►│fd=7│──►│fd=9│             │          │
│  │  └────┘   └────┘   └────┘             │          │
│  └────────────────────────────────────────┘          │
│                                                       │
└──────────────────────────────────────────────────────┘
```

---

#### epoll_ctl添加fd的流程

```
1. epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev)
    │
    ├─ 在红黑树中查找sockfd
    │   └─ 不存在
    │
    ├─ 创建epitem节点
    │   ├─ epitem->fd = sockfd
    │   ├─ epitem->events = ev.events
    │   └─ epitem->data = ev.data
    │
    ├─ 插入红黑树
    │
    └─ 注册回调函数到socket的等待队列
        └─ socket->wait_queue.add(ep_poll_callback)
```

---

#### epoll_wait的流程

```
1. epoll_wait(epfd, events, maxevents, timeout)
    │
    ├─ 检查就绪队列是否为空
    │   │
    │   ├─ 不为空 → 拷贝就绪事件到用户空间 → 返回
    │   │
    │   └─ 为空 → 阻塞等待
    │       │
    │       └─ 数据到达socket
    │           │
    │           ├─ 唤醒socket的等待队列
    │           │   └─ ep_poll_callback()
    │           │       └─ 将epitem加入就绪队列
    │           │
    │           ├─ 唤醒epoll_wait
    │           │
    │           └─ 拷贝就绪事件到用户空间 → 返回
```

---

#### 为什么epoll高效？

| 对比项 | select/poll | epoll |
|--------|------------|-------|
| **fd管理** | 每次调用都要拷贝fd_set | fd存在内核的红黑树，只需添加一次 |
| **遍历方式** | 遍历所有fd | 只处理就绪队列中的fd |
| **时间复杂度** | O(n) | O(1) |
| **fd数量限制** | 1024 (select) | 无限制 |
| **内存拷贝** | 每次全量拷贝 | 只拷贝就绪事件 |

---

## 四、TCP协议面试题

### 4.1 三次握手

```
客户端                        服务器
  │                              │
  ├─ SYN, seq=x ───────────────►│ (SYN_SENT)
  │                              │
  │                              ├─ 分配资源
  │◄─ SYN+ACK, seq=y, ack=x+1 ─┤ (SYN_RCVD)
  │                              │
  ├─ ACK, seq=x+1, ack=y+1 ────►│
  │                              │
(ESTABLISHED)              (ESTABLISHED)
```

#### Q: 为什么需要三次握手？两次不行吗？

**答案**: 防止已失效的连接请求报文突然又传到服务器

**反例场景**（两次握手）:
```
1. 客户端发送SYN1，在网络中滞留
2. 客户端超时，重发SYN2，正常建立连接，通信完毕，关闭连接
3. SYN1延迟到达服务器
4. 服务器回复SYN+ACK，进入ESTABLISHED状态
5. 🔴 客户端不认这个连接，不发数据
6. 🔴 服务器白白等待，浪费资源
```

**三次握手的好处**:
- 客户端收到SYN+ACK后，需要回复ACK
- 如果是失效的连接请求，客户端不会回复ACK
- 服务器收不到ACK，不会进入ESTABLISHED状态

---

### 4.2 四次挥手

```
客户端                        服务器
  │                              │
  ├─ FIN, seq=u ───────────────►│ (FIN_WAIT_1)
  │                              │
  │◄─ ACK, ack=u+1 ─────────────┤ (CLOSE_WAIT)
  │                              │
(FIN_WAIT_2)                     │ 继续发送数据...
  │                              │
  │◄─ FIN, seq=w, ack=u+1 ──────┤ (LAST_ACK)
  │                              │
  ├─ ACK, ack=w+1 ─────────────►│
  │                              │
(TIME_WAIT)                  (CLOSED)
  │
  │ 等待2MSL
  │
(CLOSED)
```

#### Q: 为什么是四次挥手，不是三次？

**答案**: TCP是全双工通信，双方都需要关闭

- 第一次挥手: 客户端说"我没数据要发了"
- 第二次挥手: 服务器说"我知道了"（但服务器可能还有数据要发）
- 第三次挥手: 服务器说"我也没数据要发了"
- 第四次挥手: 客户端说"我知道了"

**能否合并为三次？**
- 可以！如果服务器没有数据要发，第二次和第三次可以合并
- 但这是特殊情况，不能假设总是如此

---

#### Q: TIME_WAIT状态为什么要等待2MSL？

**MSL (Maximum Segment Lifetime)**: 报文最大生存时间（通常2分钟）

**原因1: 确保最后的ACK到达**

```
客户端                        服务器
  │                              │
  ├─ ACK, ack=w+1 ─────────────►│ (LAST_ACK)
  │                              │  ❌ ACK丢失
(TIME_WAIT)                      │
  │                              │  等待超时
  │◄─ FIN重传 ───────────────────┤
  │                              │
  ├─ ACK重传 ───────────────────►│
  │                              │
  │ 等待2MSL                     (CLOSED)
  │
(CLOSED)
```

**原因2: 确保旧连接的报文消失**

如果不等待，新连接可能收到旧连接的延迟报文。

---

### 4.3 TCP可靠传输

#### 1. 序列号和确认号

```
发送方                        接收方
  │                              │
  ├─ seq=1000, len=100 ─────────►│
  │   (数据: 1000-1099)          │
  │◄─ ack=1100 ───────────────────┤
  │   (期望收到1100)              │
  │                              │
  ├─ seq=1100, len=200 ─────────►│
  │   (数据: 1100-1299)          │
  │◄─ ack=1300 ───────────────────┤
```

#### 2. 超时重传

```
发送方                        接收方
  │                              │
  ├─ seq=1000, len=100 ─────────►│ ❌ 丢失
  │                              │
  │ 等待ACK超时 ⏰               │
  │                              │
  ├─ seq=1000, len=100 (重传) ──►│
  │                              │
  │◄─ ack=1100 ───────────────────┤
```

**RTO (Retransmission Timeout)**: 根据RTT动态计算

---

#### 3. 滑动窗口

**发送窗口**:

```
┌──────────┬──────────┬──────────┬──────────┬──────────┐
│ 已发送   │ 已发送   │ 可发送   │ 可发送   │ 不可发送 │
│ 已确认   │ 未确认   │ 未发送   │ 未发送   │          │
└──────────┴──────────┴──────────┴──────────┴──────────┘
           ▲          ▲                     ▲
           │          │                     │
       last_ack   send_next          last_ack + win
```

**流量控制**: 接收方通过ACK告知窗口大小

```
发送方                        接收方
  │                              │
  │◄─ ACK, win=1000 ─────────────┤
  │                              │
  ├─ 发送1000字节 ───────────────►│
  │                              │
  │◄─ ACK, win=500 ──────────────┤ (接收缓冲区只剩500)
  │                              │
  ├─ 发送500字节 ────────────────►│
  │                              │
  │◄─ ACK, win=0 ────────────────┤ (接收缓冲区满了)
  │                              │
  │ 停止发送 🛑                  │
```

---

### 4.4 TCP粘包/拆包

#### 问题示例

```
发送端:
  send("Hello")  // 5字节
  send("World")  // 5字节

接收端:
  recv() → "HelloWorld"  // 粘包：10字节一起收到
  或
  recv() → "Hel"         // 拆包：只收到3字节
  recv() → "loWorld"     // 拆包：收到剩余7字节
```

#### 原因

| 原因 | 说明 |
|------|------|
| **TCP是字节流** | 没有消息边界 |
| **Nagle算法** | 小包合并发送 |
| **MSS限制** | 大包拆分发送 |
| **接收缓冲区** | 数据累积 |

#### 解决方案

1. **固定长度**
   ```
   每个消息固定100字节，不足补0
   ```

2. **特殊分隔符**
   ```
   消息1\r\n消息2\r\n消息3\r\n
   ```

3. **长度前缀（最常用）**
   ```
   [4字节长度][消息体]
   例如: [0x00, 0x00, 0x00, 0x05]["Hello"]
   ```

4. **应用层协议**
   - HTTP: Content-Length
   - WebSocket: Frame头部包含长度
   - Protobuf: varint编码长度

---

## 五、高并发服务器设计

### 5.1 C10K问题

**定义**: 单机如何支持10000个并发连接？

#### 传统方案的瓶颈

| 方案 | 瓶颈 | 最大并发 |
|------|------|---------|
| **每连接一个进程** | 进程开销大（内存、上下文切换） | ~100 |
| **每连接一个线程** | 线程开销（栈空间、上下文切换） | ~1000 |
| **select/poll** | O(n)遍历，fd数量限制 | ~1024 |

#### 解决方案

| 技术 | 说明 |
|------|------|
| **epoll** | O(1)时间复杂度，无fd数量限制 |
| **非阻塞I/O** | 避免线程阻塞 |
| **Reactor模式** | 事件驱动，一个线程处理多个连接 |
| **线程池** | 复用线程，减少创建开销 |

---

### 5.2 常见并发模型

#### 1. 单线程Reactor（Redis）

```
┌──────────────────────────────────────────────────────┐
│  主线程                                               │
│                                                       │
│  EventLoop                                           │
│    ├─ accept() → connfd                              │
│    ├─ read(connfd1)                                  │
│    │   └─ process() → write(connfd1)                 │
│    ├─ read(connfd2)                                  │
│    │   └─ process() → write(connfd2)                 │
│    └─ ...                                            │
└──────────────────────────────────────────────────────┘
```

**特点**:
- ✅ 实现简单，无锁
- ✅ 适合计算量小的场景（如Redis的内存操作）
- ❌ 单核，无法利用多核CPU
- ❌ 一个慢请求阻塞所有连接

---

#### 2. 多线程Reactor（Muduo, Netty）

```
┌──────────────────────────────────────────────────────┐
│  主线程 (Acceptor线程)                                │
│                                                       │
│  EventLoop                                           │
│    └─ accept() → connfd                              │
│        └─ 分派到子线程 ──┐                           │
└──────────────────────────┼──────────────────────────┘
                           │
             ┌─────────────┼─────────────┐
             ▼             ▼             ▼
┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
│ 子线程1         │ │ 子线程2         │ │ 子线程N         │
│                 │ │                 │ │                 │
│ EventLoop       │ │ EventLoop       │ │ EventLoop       │
│  ├─ read(conn1) │ │  ├─ read(conn2) │ │  ├─ read(connN) │
│  ├─ process()   │ │  ├─ process()   │ │  ├─ process()   │
│  └─ write()     │ │  └─ write()     │ │  └─ write()     │
└─────────────────┘ └─────────────────┘ └─────────────────┘
```

**特点**:
- ✅ 充分利用多核
- ✅ 主线程只负责Accept，快速响应
- ✅ 每个连接只在一个线程处理，避免锁

---

#### 3. 多Reactor + 线程池（Nginx）

```
┌──────────────────────────────────────────────────────┐
│  主Reactor线程                                        │
│    └─ accept() → connfd                              │
│        └─ 分派到子Reactor ──┐                        │
└──────────────────────────────┼──────────────────────┘
                               │
             ┌─────────────────┼─────────────┐
             ▼                 ▼             ▼
┌─────────────────┐     ┌─────────────────┐
│ 子Reactor1      │     │ 子Reactor2      │
│                 │     │                 │
│ EventLoop       │     │ EventLoop       │
│  ├─ read()      │     │  ├─ read()      │
│  ├─ 投递到 ────┐│     │  └─ 投递到 ────┐│
│  └─ write()    ││     │                 ││
└────────────────┘│     └─────────────────┘│
                  │                         │
                  └─────────────┬───────────┘
                                ▼
┌──────────────────────────────────────────────────────┐
│  线程池 (计算密集型任务)                              │
│    ├─ Worker1: process()                             │
│    ├─ Worker2: process()                             │
│    └─ Worker3: process()                             │
└──────────────────────────────────────────────────────┘
```

**特点**:
- ✅ I/O和计算分离
- ✅ 适合耗时操作（如数据库查询）
- ⚠️ 需要考虑任务队列大小

---

### 5.3 性能优化技巧

#### 1. 减少系统调用

| 技术 | 说明 |
|------|------|
| **sendfile** | 零拷贝，文件直接发送到socket |
| **mmap** | 内存映射，减少内核/用户空间拷贝 |
| **writev** | 聚集写，一次写多个buffer |
| **readv** | 分散读，一次读到多个buffer |

#### 2. 减少内存拷贝

```
传统方式（4次拷贝）:
  磁盘 → 内核缓冲区 → 用户空间 → socket缓冲区 → 网卡

sendfile（2次拷贝）:
  磁盘 → 内核缓冲区 → 网卡
```

#### 3. TCP参数调优

```bash
# 增大监听队列
net.core.somaxconn = 4096

# 增大TCP接收/发送缓冲区
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216

# TIME_WAIT快速回收
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1  # 不推荐，可能导致问题

# TCP keepalive
net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_keepalive_probes = 3
```

---

## 六、muduo网络库常见面试题

### Q1: muduo如何实现One Loop Per Thread？

**答案**:

1. **EventLoop构造时记录线程ID**
   ```cpp
   EventLoop::EventLoop()
       : threadId_(CurrentThread::tid())  // 记录创建线程
   { }
   ```

2. **thread_local确保每线程只有一个EventLoop**
   ```cpp
   __thread EventLoop* t_loopInThisThread = nullptr;

   EventLoop::EventLoop() {
       if (t_loopInThisThread) {
           LOG_FATAL << "Another EventLoop exists in this thread";
       } else {
           t_loopInThisThread = this;
       }
   }
   ```

3. **线程安全检查**
   ```cpp
   void EventLoop::assertInLoopThread() {
       if (threadId_ != CurrentThread::tid()) {
           abortNotInLoopThread();
       }
   }
   ```

---

### Q2: muduo如何实现跨线程唤醒？

**答案**: `eventfd` + `wakeupChannel_`

```cpp
// 1. 创建eventfd
wakeupFd_ = ::eventfd(0, EFD_NONBLOCK | EFD_CLOEXEC);

// 2. 注册到epoll
wakeupChannel_->setReadCallback(
    std::bind(&EventLoop::handleRead, this));
wakeupChannel_->enableReading();

// 3. 其他线程唤醒
void EventLoop::wakeup() {
    uint64_t one = 1;
    ::write(wakeupFd_, &one, sizeof one);  // 写eventfd
}

// 4. EventLoop被唤醒
void EventLoop::handleRead() {
    uint64_t one = 1;
    ::read(wakeupFd_, &one, sizeof one);  // 读eventfd
}
```

---

### Q3: muduo的Buffer为什么用readv？

**答案**: **一次系统调用 + 按需扩容**

```cpp
ssize_t Buffer::readFd(int fd, int* savedErrno) {
    char extrabuf[65536];  // 栈上临时缓冲区
    struct iovec vec[2];

    // 第一块：Buffer的writable区域
    vec[0].iov_base = beginWrite();
    vec[0].iov_len = writableBytes();

    // 第二块：栈上临时缓冲区
    vec[1].iov_base = extrabuf;
    vec[1].iov_len = sizeof extrabuf;

    // 一次系统调用读取到两块内存
    const ssize_t n = ::readv(fd, vec, 2);

    if (n <= writableBytes()) {
        writerIndex_ += n;  // Buffer够用
    } else {
        writerIndex_ = buffer_.size();
        append(extrabuf, n - writableBytes());  // extrabuf的数据追加到Buffer
    }

    return n;
}
```

---

### Q4: TcpConnection的生命周期如何管理？

**答案**: `shared_ptr` + `weak_ptr` + `Channel::tie()`

1. **TcpConnection由shared_ptr管理**
   ```cpp
   typedef std::shared_ptr<TcpConnection> TcpConnectionPtr;
   ```

2. **TcpServer持有一份shared_ptr**
   ```cpp
   std::map<string, TcpConnectionPtr> connections_;
   ```

3. **Channel持有weak_ptr，防止循环引用**
   ```cpp
   void TcpConnection::connectEstablished() {
       channel_->tie(shared_from_this());  // weak_ptr
   }
   ```

4. **事件处理时提升为shared_ptr**
   ```cpp
   void Channel::handleEvent(Timestamp receiveTime) {
       std::shared_ptr<void> guard;
       if (tied_) {
           guard = tie_.lock();  // 提升为shared_ptr
           if (guard) {
               handleEventWithGuard(receiveTime);  // 保证对象不被销毁
           }
       }
   }
   ```

---

### Q5: muduo为什么不用ET模式？

**答案**:

1. **LT更简单**: 不用担心丢事件
2. **LT更安全**: 可以按需读取，不必一次读完
3. **性能差距不大**: 对于大多数应用，LT性能已经足够
4. **muduo的设计哲学**: 简单、正确、高效（按此优先级）

**muduo作者陈硕的观点**:
> LT的编程复杂度更低，不容易漏掉事件，也不容易发生漏收数据的bug。
> 对于大数据量高吞吐的应用，epoll本身并不是瓶颈。

---

## 七、项目实战问题

### Q1: 你的项目中如何处理半包和粘包？

**答案**（以WebSocket为例）:

```cpp
// 1. WebSocket消息格式
// ┌──────────┬──────────────┬────────────────┐
// │ Frame头  │ Payload长度  │ Payload数据    │
// │ (2-14B)  │ (7/16/64位)  │                │
// └──────────┴──────────────┴────────────────┘

void CWebSocketConn::OnRead(Buffer* buf) {
    while (buf->readableBytes() > 0) {
        // 步骤1: 解析Frame头
        if (buf->readableBytes() < 2) {
            break;  // 半包，等待更多数据
        }

        uint8_t byte1 = buf->peek()[0];
        uint8_t byte2 = buf->peek()[1];

        bool fin = (byte1 & 0x80) != 0;
        uint8_t opcode = byte1 & 0x0F;
        bool mask = (byte2 & 0x80) != 0;
        uint64_t payload_length = byte2 & 0x7F;

        // 步骤2: 解析Payload长度
        size_t header_len = 2;
        if (payload_length == 126) {
            if (buf->readableBytes() < 4) break;  // 半包
            payload_length = ntohs(*(uint16_t*)(buf->peek() + 2));
            header_len = 4;
        } else if (payload_length == 127) {
            if (buf->readableBytes() < 10) break;  // 半包
            payload_length = be64toh(*(uint64_t*)(buf->peek() + 2));
            header_len = 10;
        }

        // 步骤3: 检查是否接收完整
        size_t frame_len = header_len + (mask ? 4 : 0) + payload_length;
        if (buf->readableBytes() < frame_len) {
            break;  // 半包，等待更多数据
        }

        // 步骤4: 提取完整消息
        string frame_data = buf->retrieveAsString(frame_len);

        // 步骤5: 处理消息
        processWebSocketFrame(frame_data);
    }
}
```

**关键点**:
- 检查消息是否完整
- 不完整就break，等待下次读取
- 使用Buffer累积数据

---

### Q2: 如何保证高并发下的线程安全？

**答案**（以ConnectionManager为例）:

```cpp
class ConnectionManager {
    std::atomic<uint32_t> m_next_id{1};  // ✅ 原子操作
    std::shared_mutex m_mutex;            // ✅ 读写锁
    std::unordered_map<uint32_t, HttpHandlerPtr> m_connections;

public:
    // 添加连接（写操作，独占锁）
    uint32_t AddConnection(const HttpHandlerPtr& handler) {
        uint32_t id = m_next_id.fetch_add(1);  // 无锁
        std::unique_lock<std::shared_mutex> lock(m_mutex);  // 写锁
        m_connections[id] = handler;
        return id;
    }

    // 查询连接（读操作，共享锁）
    HttpHandlerPtr GetConnection(uint32_t id) {
        std::shared_lock<std::shared_mutex> lock(m_mutex);  // 读锁（多个线程可同时读）
        auto it = m_connections.find(id);
        return (it != m_connections.end()) ? it->second : nullptr;
    }
};
```

**线程安全策略**:
1. **ID生成**: 使用`std::atomic`，无需加锁
2. **添加/删除**: 使用`unique_lock`（写锁），独占访问
3. **查询**: 使用`shared_lock`（读锁），允许并发读取

---

### Q3: 如何处理大量TIME_WAIT？

**答案**:

#### 原因分析
```
主动关闭的一方会进入TIME_WAIT状态，等待2MSL（~2分钟）
如果服务器主动关闭大量连接，会产生大量TIME_WAIT
```

#### 解决方案

1. **让客户端主动关闭**
   ```cpp
   // 服务器先shutdown写半部
   conn->shutdown();  // 不是close

   // 客户端收到FIN后关闭
   // TIME_WAIT在客户端，不影响服务器
   ```

2. **SO_REUSEADDR**
   ```cpp
   int reuse = 1;
   setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &reuse, sizeof(reuse));
   ```

3. **TCP参数调优**
   ```bash
   # 允许TIME_WAIT状态的socket重用（谨慎使用）
   net.ipv4.tcp_tw_reuse = 1

   # 减少TIME_WAIT时间（不推荐，可能违反TCP规范）
   # net.ipv4.tcp_tw_recycle = 1  # 已废弃
   ```

4. **增加端口范围**
   ```bash
   # 增加可用端口
   net.ipv4.ip_local_port_range = 10000 65535
   ```

---

### Q4: 如何实现服务器优雅退出？

**答案**:

```cpp
class GracefulShutdown {
    std::atomic<bool> shutdown_requested_{false};
    EventLoop* loop_;

public:
    void requestShutdown() {
        shutdown_requested_.store(true);
        loop_->quit();  // 退出事件循环
    }

    void shutdown() {
        // 1. 停止接受新连接
        acceptor_->stop();

        // 2. 等待正在处理的请求完成
        std::this_thread::sleep_for(std::chrono::seconds(5));

        // 3. 关闭所有连接
        for (auto& conn : connections_) {
            conn.second->forceClose();
        }

        // 4. 清理资源
        threadPool_->stop();
        redis_pool_->close();
        mysql_pool_->close();
    }
};

// 信号处理
void signalHandler(int sig) {
    if (sig == SIGINT || sig == SIGTERM) {
        server.requestShutdown();
    }
}

int main() {
    signal(SIGINT, signalHandler);
    signal(SIGTERM, signalHandler);

    server.start();
    server.loop();

    // 收到信号后
    server.shutdown();
}
```

---

### Q5: 如何监控服务器性能？

**答案**:

```cpp
class ServerMetrics {
    std::atomic<uint64_t> total_connections_{0};
    std::atomic<uint64_t> active_connections_{0};
    std::atomic<uint64_t> total_requests_{0};
    std::atomic<uint64_t> total_bytes_received_{0};
    std::atomic<uint64_t> total_bytes_sent_{0};

public:
    void recordNewConnection() {
        total_connections_.fetch_add(1);
        active_connections_.fetch_add(1);
    }

    void recordCloseConnection() {
        active_connections_.fetch_sub(1);
    }

    void recordRequest(size_t bytes_received, size_t bytes_sent) {
        total_requests_.fetch_add(1);
        total_bytes_received_.fetch_add(bytes_received);
        total_bytes_sent_.fetch_add(bytes_sent);
    }

    // 定时上报到监控系统
    void report() {
        LOG_INFO << "Total connections: " << total_connections_;
        LOG_INFO << "Active connections: " << active_connections_;
        LOG_INFO << "Total requests: " << total_requests_;
        LOG_INFO << "QPS: " << qps();
    }
};
```

**监控指标**:
- 连接数：总连接数、活跃连接数
- 吞吐量：QPS、字节数/秒
- 延迟：平均延迟、P99延迟
- 错误率：错误请求数/总请求数
- CPU/内存使用率

---

## 八、面试技巧

### 8.1 回答框架

#### STAR法则

| 维度 | 说明 | 示例 |
|------|------|------|
| **Situation** | 项目背景 | "我们的聊天室项目需要支持10000+并发连接" |
| **Task** | 面临的任务/问题 | "传统的一个连接一个线程的模型无法满足需求" |
| **Action** | 采取的行动 | "使用了muduo网络库的Reactor多线程模型" |
| **Result** | 结果 | "最终支持了20000并发连接，CPU利用率90%" |

---

### 8.2 常见追问

#### Q: 为什么选择muduo而不是Boost.Asio？

**答案**:
- muduo更轻量，只关注网络I/O
- muduo的Reactor模型更容易理解和调试
- muduo的源码质量高，适合学习
- Boost.Asio更复杂，学习曲线陡峭

#### Q: muduo有什么缺点？

**答案**:
- 不跨平台（Linux only）
- 功能相对简单（如不支持HTTP/2）
- 文档较少（主要靠看源码）

#### Q: 如果让你改进muduo，你会怎么做？

**答案**:
- 支持io_uring（Linux 5.1+的新异步I/O）
- 添加更多应用层协议（HTTP/2, gRPC）
- 改进日志系统（支持结构化日志）
- 添加更多性能监控指标

---

**文档生成时间**: 2025-11-03
**项目路径**: `/home/utopianyouth/zerovoice/unit11/myself-chatroom-distribute`

---

## 推荐阅读

1. 《UNIX网络编程 卷1: 套接字联网API》 - W. Richard Stevens
2. 《TCP/IP详解 卷1: 协议》 - W. Richard Stevens
3. 《Linux多线程服务端编程》 - 陈硕
4. 《高性能服务器编程》- 游双
5. Muduo源码: https://github.com/chenshuo/muduo
6. Linux man pages: `man epoll`, `man tcp`, `man socket`
