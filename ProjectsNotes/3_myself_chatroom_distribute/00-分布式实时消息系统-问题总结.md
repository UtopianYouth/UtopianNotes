# 分布式实时消息推送系统 - 面试准备文档

## 目录
- [一、项目概述](#一项目概述)
- [二、技术架构](#二技术架构)
- [三、设计模式应用](#三设计模式应用)
- [四、中间件使用详解](#四中间件使用详解)
- [五、网络编程核心](#五网络编程核心)
- [六、技术难点与创新](#六技术难点与创新)
- [七、高频面试题](#七高频面试题)

---

## 一、项目概述

### 1.1 项目介绍
分布式实时消息推送系统，采用**三层微服务架构**，支持WebSocket长连接、房间隔离消息、全局广播等功能。

**技术栈:**
- **网络库:** Muduo (基于Reactor模式)
- **RPC:** gRPC + Protocol Buffers
- **消息队列:** Kafka
- **缓存:** Redis
- **数据库:** MySQL
- **部署:** Docker + docker-compose

### 1.2 核心架构

```
客户端(WebSocket)
    ↓
Comet层(chat-room) - 长连接网关，端口8080
    ↓ HTTP转发          ↑ gRPC推送
Logic层 - 业务处理，端口8090
    ↓ Kafka生产
Kafka消息队列 - 端口9092
    ↓ 消费
Job层 - 消息推送
    ↑ gRPC调用(50051)
Comet层广播 → 客户端
```

**职责划分:**
- **Comet层**: 维护WebSocket连接，接收客户端消息，推送消息
- **Logic层**: 用户认证，业务逻辑处理，数据持久化(MySQL/Redis)
- **Job层**: 消费Kafka消息，通过gRPC调用Comet层推送

---

## 二、技术架构

### 2.1 三层解耦设计

**为什么采用三层架构?**
1. **职责分离** - 每层专注核心功能，单一职责原则
2. **独立扩展** - 高并发时可只扩展Comet层
3. **解耦隔离** - Kafka解耦，某层故障不影响其他层
4. **技术灵活** - 每层可选最适合的技术栈

**扩展方案(支持100万并发):**
```
1. Comet层水平扩展 - 10个节点 × 10万连接
2. Nginx负载均衡 - 4层/7层LB
3. Redis记录映射 - user_id -> comet_server
4. Kafka分区扩展 - 增加分区数提升吞吐
5. MySQL读写分离 - 主从复制 + 分库分表
```

### 2.2 通信协议

**后端Protobuf协议** (`ChatRoom.Protocol.proto`)
```protobuf
message Proto {
    int32 ver = 1;     // 协议版本
    int32 op = 2;      // 操作码: 4=房间消息, 5=创建房间
    int32 seq = 3;     // 序列号
    bytes body = 4;    // JSON或Protobuf消息体
}
```

**前后端JSON协议**
```json
// 客户端 → Comet
{"type": "clientMessages", "payload": {...}}
{"type": "requestRoomHistory", "payload": {...}}
{"type": "clientCreateRoom", "payload": {...}}

// Comet → 客户端
{"type": "hello", "payload": {...}}
{"type": "serverMessages", "payload": {...}}
{"type": "serverRoomHistory", "payload": {...}}
```

---

## 三、设计模式应用

### 3.1 单例模式 (Singleton)

**应用场景:** 全局唯一资源管理

**数据库连接池管理器** (`logic/mysql/db_pool.cc:519`)

```cpp
CDBManager *CDBManager::getInstance() {
    if (!s_db_manager) {
        s_db_manager = new CDBManager();
        if (s_db_manager->Init()) {
            delete s_db_manager;
            s_db_manager = NULL;
        }
    }
    return s_db_manager;
}
```

**房间服务** (`logic/service/room_service.h:18`)
```cpp
static RoomService& getInstance() {
    static RoomService instance;  // Meyer's Singleton (C++11线程安全，使用了C++11标准)
    return instance;
}
```

**面试要点:**
- Meyer's Singleton线程安全性: C++11保证static局部变量初始化线程安全
- 懒汉式 vs 饿汉式: 懒汉式延迟初始化，饿汉式程序启动时创建
- 为何用单例: 全局唯一资源(如连接池管理器)避免重复创建

### 3.2 对象池模式 (Object Pool)

**数据库连接池** (`logic/mysql/db_pool.cc:377-613`)

**核心机制:**
```cpp
// 1. 初始化最小连接数
int CDBPool::Init() {
    for (int i = 0; i < db_cur_conn_cnt_; i++) {
        CDBConn *db_conn = new CDBConn(this);
        db_conn->Init();
        free_list_.push_back(db_conn);
    }
}

// 2. 获取连接 - 支持超时等待和动态扩容
CDBConn *CDBPool::GetDBConn(const int timeout_ms) {
    std::unique_lock<std::mutex> lock(mutex_);

    if (free_list_.empty()) {
        if (db_cur_conn_cnt_ >= db_max_conn_cnt_) {
            // 达到最大连接数，条件变量等待
            cond_var_.wait_for(lock,
                std::chrono::milliseconds(timeout_ms),
                [this] { return (!free_list_.empty()) | abort_request_; });
        } else {
            // 动态创建新连接
            CDBConn *db_conn = new CDBConn(this);
            db_conn->Init();
            free_list_.push_back(db_conn);
            db_cur_conn_cnt_++;
        }
    }

    CDBConn *pConn = free_list_.front();
    free_list_.pop_front();
    return pConn;
}

// 3. 归还连接
void CDBPool::RelDBConn(CDBConn *pConn) {
    std::lock_guard<std::mutex> lock(mutex_);
    free_list_.push_back(pConn);
    cond_var_.notify_one();  // 通知等待线程
}
```

**面试要点:**
- **为何需要连接池?** 减少连接创建/销毁开销，复用连接
- **如何处理连接耗尽?** 条件变量等待 + 超时机制
- **动态扩容策略?** 当前连接数 < 最大连接数时创建新连接
- **线程安全保证?** mutex + condition_variable
- **数据结构选择?** std::list频繁删除O(1)，只需front()取头部

### 3.3 发布-订阅模式 (Pub/Sub)

**房间订阅管理** (`chat-room/service/pub_sub_service.cc`)
```cpp
class RoomTopic {
    string room_id;
    string room_topic;
    std::unordered_set<string> user_ids;  // 订阅者集合

    void AddSubscriber(const string& user_id);
    void DeleteSubscriber(const string& user_id);
    std::unordered_set<string>& GetSubscribers();
};
```

**应用场景:**
- Comet层维护房间-用户订阅关系
- 消息到达时查找订阅者列表广播
- unordered_set快速查找订阅者O(1)

---

## 四、中间件使用详解

### 4.1 MySQL核心技术

#### 4.1.1 连接保活机制

**问题:** MySQL默认8小时无操作断开连接(wait_timeout=28800s)

**解决方案** (`logic/mysql/db_pool.cc:225-237`)

```cpp
bool CDBConn::CheckAndReconnect() {
    if (mysql_ping(mysql_) != 0) {  // 检测连接是否存活
        LOG_WARN << "mysql connection lost, reconnecting...";
        mysql_close(mysql_);
        mysql_ = NULL;
        if (Init() != 0) {
            LOG_ERROR << "reconnect failed";
            return false;
        }
        LOG_INFO << "reconnect success";
    }
    return true;
}

// 每次执行SQL前调用
bool CDBConn::ExecuteUpdate(const char *sql_query) {
    if (!CheckAndReconnect()) return false;
    if (mysql_real_query(mysql_, sql_query, strlen(sql_query))) {
        LOG_ERROR << "mysql_real_query failed";
        return false;
    }
    return true;
}
```

**面试要点:**
- **mysql_ping原理?** 发送COM_PING命令，连接存活返回0，断开则自动重连
- **为何每次执行前检查?** Docker环境下连接易断开，防止执行失败
- **MySQL 8.0变化?** 废弃auto_reconnect，需手动重连

#### 4.1.2 Prepared Statement防SQL注入

**实现** (`logic/mysql/db_pool.cc:87-115`)
```cpp
bool CPrepareStatement::Init(MYSQL *mysql, string &sql) {
    mysql_ping(mysql);  // 先检测连接
    stmt_ = mysql_stmt_init(mysql);
    mysql_stmt_prepare(stmt_, sql.c_str(), sql.size());
    param_cnt_ = mysql_stmt_param_count(stmt_);
    param_bind_ = new MYSQL_BIND[param_cnt_];
}

void CPrepareStatement::SetParam(uint32_t index, string &value) {
    param_bind_[index].buffer_type = MYSQL_TYPE_STRING;
    param_bind_[index].buffer = (char *)value.c_str();
    param_bind_[index].buffer_length = value.size();
}

bool CPrepareStatement::ExecuteUpdate() {
    if (mysql_stmt_bind_param(stmt_, param_bind_)) return false;
    if (mysql_stmt_execute(stmt_)) return false;
    return true;
}
```

**面试要点:**
- **如何防SQL注入?** 参数和SQL分离，参数不会被当作SQL执行
- **绑定机制?** mysql_stmt_bind_param指定类型，服务端验证并转义

#### 4.1.3 连接池设计

**最大连接数设置:**
```
公式: connections = (core_count × 2) + effective_spindle_count
考虑因素: CPU核心数、磁盘IO能力、MySQL max_connections限制
监控调优: 观察连接数使用率，动态调整
```

**事务支持** (`logic/mysql/db_pool.cc:336-373`)
```cpp
bool StartTransaction();  // BEGIN
bool Commit();           	// COMMIT
bool Rollback();         	// ROLLBACK
```

### 4.2 Redis核心技术

#### 4.2.1 数据结构选型

| 数据结构 | 使用场景 | 命令 | 代码位置 |
|---------|---------|------|---------|
| String | Cookie-Email映射 | SET/GET/SETEX | cache_pool.h:55-57 |
| List | 房间历史消息队列 | LPUSH/LRANGE | cache_pool.h:78-82 |
| Hash | 用户信息，房间元数据 | HSET/HGET/HMGET | cache_pool.h:66-76 |
| ZSet | 消息队列优先级 | ZADD/ZREVRANGE | cache_pool.h:84-92 |

#### 4.2.2 历史消息缓存策略

**实现** (`logic/service/message_service.h:24-26`)
```cpp
// 热点数据优先从Redis读取
int getRoomHistory(Room& room, MessageBatch& message_batch,
                  const int msg_count = k_message_batch_size);
```

**流程:**
```
1. LRANGE room:history start end (从Redis List读取)
2. 命中 → 返回结果
3. 未命中 → MySQL回源查询
4. 回源数据 → LPUSH写回Redis(缓存预热)
```

**面试要点:**
- **为何用List?** 符合时间序列特性，LRANGE范围查询高效
- **缓存击穿防止?** 互斥锁(setnx) + 缓存预热
- **数据一致性?** 先写MySQL，再写Redis (Cache-Aside模式)

#### 4.2.3 Redis持久化

**RDB vs AOF:**
```
RDB: 快照，fork子进程dump内存，适合备份，恢复快
AOF: 追加日志，记录每个写命令，数据更完整
选择:
  - 允许丢失几分钟数据: RDB
  - 不能丢数据: AOF + everysec
  - 生产环境: RDB + AOF混合持久化
```

### 4.3 Kafka核心技术

#### 4.3.1 生产者/消费者实现

**生产者** (`logic/service/kafka_producer.h`)
```cpp
class KafkaProducer {
    bool init(const std::string& brokers, const std::string& topic);
    bool sendMessage(const std::string& message);  // 异步发送
private:
    static void deliveryCallback(RdKafka::Message& message, void* opaque);
    RdKafka::Producer* producer_;
};
```

**消费者** (`job/service/kafka_consumer.h`)
```cpp
class KafkaConsumer {
    KafkaConsumer(const std::string& brokers, const std::string& topic,
                 const std::string& group_id = "my_consumer_group");
    std::string consume(int timeout_ms = 1000);  // 轮询消费
private:
    std::unique_ptr<RdKafka::KafkaConsumer> consumer_;
};
```

#### 4.3.2 消息格式

**Protobuf定义** (`logic/proto/ChatRoom.Job.proto`)

```protobuf
message PushMsg {
    enum Type {
        PUSH = 0;         // 推送给指定用户
        ROOM = 1;         // 推送给指定房间
        BROADCAST = 2;    // 广播给所有人
    }
    Type type = 1;
    int32 operation = 2;  // 操作码: 4=房间消息, 5=创建房间
    string room = 5;      // 房间ID
    repeated string keys = 6;  // 目标用户ID列表
    bytes msg = 7;        // JSON序列化的消息体
}
```

#### 4.3.3 消息可靠性保证

**生产者端:**
```cpp
acks=all          // 等待所有ISR副本确认
retries>0         // 失败自动重试
enable.idempotence=true  // 幂等性，防止重复
```

**Broker端:**
```cpp
replication.factor>=3     // 多副本
min.insync.replicas>=2    // 最少同步副本数
```

**消费者端:**
```cpp
enable.auto.commit=false  // 手动提交offset
// 处理完成后再提交，避免消息丢失
```

**面试要点:**
- **为何选Kafka?** 高吞吐量(10万+/s)，削峰填谷，异步解耦
- **分区策略?** 使用room_id作为key，保证同房间消息顺序
- **Consumer Group?** 同组内消费者共享分区，负载均衡
- **ISR机制?** In-Sync Replicas，只有ISR中的副本可成为Leader

---

## 五、网络编程核心

### 5.1 Muduo网络库

#### 5.1.1 核心组件

```apl
EventLoop - Reactor模式事件循环 (one loop per thread)
TcpServer - 监听连接，管理TcpConnection
TcpConnection - 封装socket，提供Buffer缓冲区
ThreadPool - 业务逻辑处理线程池
```

#### 5.1.2 线程模型

**实现** (`chat-room/main.cc:121-150`)
```cpp
class HttpServer {
    HttpServer(EventLoop* loop, const InetAddress& addr,
               const ServerConfig& config)
        : m_server(loop, addr, name) {
        m_server.setConnectionCallback(...);
        m_server.setMessageCallback(...);
        m_server.setThreadNum(m_config.num_event_loops);  // IO线程数
    }

    bool Start() {
        CWebSocketConn::InitThreadPool(m_config.num_threads); // 业务线程池
        m_server.start();
    }
};
```

**线程架构:**

```
主线程(EventLoop)
    ↓ accept新连接
IO线程池(EventLoopThreadPool)
    ↓ read/write
业务线程池(ThreadPool)
    ↓ 处理业务逻辑
```

**面试要点:**
- **Reactor vs Proactor?** Muduo采用Reactor，应用程序负责read/write
- **one loop per thread?** 每个IO线程独立EventLoop，避免锁竞争
- **epoll ET vs LT?** Muduo使用LT模式，编程简单不易出错

### 5.2 WebSocket协议

#### 5.2.1 握手过程

```
1. 客户端发送HTTP Upgrade请求 (携带Cookie认证)
2. 服务端提取Cookie，POST /logic/verify验证
3. 验证成功: 计算Sec-WebSocket-Accept返回101
4. 升级为WebSocket连接，发送hello帧
5. 验证失败: 发送Close帧(code=1008)，断开连接
```

**Sec-WebSocket-Accept计算:**
```
Base64(SHA1(Sec-WebSocket-Key + "258EAFA5-E914-47DA-95CA-C5AB0DC85B11"))
```

#### 5.2.2 帧格式

**构建WebSocket帧** (`chat-room/service/websocket_conn.cc`)
```cpp
string BuildWebSocketFrame(const string& payload, uint8_t opcode = 0x01) {
    // FIN=1, opcode=0x01(text frame)
    // mask=0 (server->client不需要mask)
    // payload length编码 (7位/16位/64位)
}
```

**连接管理** (`chat-room/service/websocket_conn.h:11-36`)
```cpp
class CWebSocketConn : public CHttpConn {
private:
    string user_id;
    string username;
    bool handshake_completed = false;
    std::unordered_map<string, Room> rooms_map;  // 已加入房间
    string incomplete_frame_buffer;              // 粘包处理
    static ThreadPool* s_thread_pool;
};
```

**面试要点:**
- **WebSocket vs HTTP轮询?** 双向通信，减少延迟，节省带宽
- **为何服务端不mask?** RFC6455规定只有客户端→服务端需要mask
- **粘包半包处理?** incomplete_frame_buffer缓存不完整帧

### 5.3 gRPC通信

#### 5.3.1 服务定义

**Protobuf** (`chat-room/rpc/ChatRoom.Comet.proto`)

```protobuf
service Comet {
    rpc PushMsg(PushMsgReq) returns (PushMsgReply);        // 单播
    rpc Broadcast(BroadcastReq) returns (BroadcastReply);  // 广播
    rpc BroadcastRoom(BroadcastRoomReq) returns (BroadcastRoomReply); // 房间广播
    rpc Rooms(RoomsReq) returns (RoomsReply);
}
```

#### 5.3.2 客户端实现

**重试机制** (`job/main.cc:20-136`)
```cpp
class CometClient {
    bool broadcastRoom(const std::string& roomId, const std::string& msgContent) {
        const int MAX_RETRIES = 3;

        for (int retry = 0; retry < MAX_RETRIES; ++retry) {
            grpc::ClientContext context;
            context.set_deadline(std::chrono::system_clock::now() + 5s);

            grpc::Status status = stub_->BroadcastRoom(&context, request, &response);
            if (status.ok()) return true;

            // 重试逻辑
            if (status.error_code() == grpc::StatusCode::UNAVAILABLE) {
                recreateConnection();  // 重建连接
            }

            std::this_thread::sleep_for(std::chrono::milliseconds(100 * (retry + 1)));
        }
        return false;
    }

    void recreateConnection() {
        auto channel = grpc::CreateChannel(server_address_,
                                          grpc::InsecureChannelCredentials());
        stub_ = ChatRoom::Comet::Comet::NewStub(channel);
    }
};
```

**面试要点:**
- **gRPC vs REST?** 基于HTTP/2，支持流式，Protobuf高效序列化
- **超时处理?** set_deadline + 重试机制
- **连接断开?** 检测UNAVAILABLE错误码，重建连接

---

## 六、技术难点与创新

### 6.1 核心技术难点

#### 1. MySQL连接自动重连

**问题:** Docker容器环境下MySQL默认8小时无操作断开连接

**解决:**

- 每次执行SQL前调用`CheckAndReconnect()`
- `mysql_ping`检测连接，断开则重新初始化
- 代码位置: `logic/mysql/db_pool.cc:88, 225, 240, 254, 268, 281`

#### 2. Redis历史消息查询优化

**问题:** 前端滚动加载历史消息时，锚点消息索引传递错误

**解决:**
- 使用`firstMessageId`作为锚点
- 基于此消息在Redis List中查找历史记录
- LRANGE范围查询优化

#### 3. Nginx长连接保持

**问题:** Nginx默认60s无操作断开TCP连接

**解决:**
```nginx
proxy_read_timeout 600s;
proxy_send_timeout 600s;
keepalive_timeout 600s;
```

#### 4. Kafka/Zookeeper内存限制

**问题:** 云服务器资源有限，Kafka占用内存过大

```yaml
# docker-compose.yml
zookeeper:
  environment:
    - KAFKA_HEAP_OPTS="-Xmx512M -Xms512M"
kafka:
  environment:
    - KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
```

#### 5. 数据库连接池线程安全

**难点:** 多线程环境下连接池的获取/归还需要同步

```cpp
CDBConn *CDBPool::GetDBConn(const int timeout_ms) {
    std::unique_lock<std::mutex> lock(mutex_);
    // 条件变量等待空闲连接
    cond_var_.wait_for(lock, std::chrono::milliseconds(timeout_ms),
        [this] { return (!free_list_.empty()) | abort_request_; });
    // ...
}
```

### 6.2 项目创新点

1. **三层架构完全解耦** - Comet/Logic/Job通过Kafka解耦，可独立扩展
2. **Redis + MySQL双层存储** - 热点数据Redis，冷数据MySQL，缓存未命中自动回源
3. **gRPC服务发现与重连** - 检测UNAVAILABLE自动重建连接
4. **Docker容器化部署** - docker-compose一键启动，支持云服务器
5. **完整用户认证流程** - Cookie-Session机制，WebSocket握手验证

---

## 七、高频面试题

### 架构设计类

**Q1: 为什么采用三层架构而不是两层?**

- 职责分离: Comet专注长连接，Logic专注业务，Job专注推送
- 独立扩展: 高并发时可只扩展Comet层
- 解耦隔离: Kafka解耦，某层故障不影响其他层
- 技术灵活: 每层可选最适合的技术栈

**Q2: 如果要支持100万并发连接，如何扩展?**

- Comet层水平扩展: 部署10个节点，每节点承载10万连接
- 负载均衡: Nginx/LVS做4层或7层负载均衡
- 连接信息存储: Redis记录user_id→comet_server映射
- Job层改进: 推送时查询Redis获取用户所在Comet节点
- Kafka分区扩展: 增加分区数，提升消息吞吐量

**Q3: 如何保证消息的可靠投递?**

- 生产者: Kafka acks=all，等待所有副本确认
- 消费者: 手动提交offset，处理成功后再提交
- 存储: 先写MySQL，再发Kafka，双重保障
- 重试机制: gRPC调用失败3次重试
- 监控告警: 消费lag监控，异常及时告警

**Q4: WebSocket连接断开如何处理?**

- 客户端重连: 断线后自动重连，携带last_message_id
- 消息补偿: 重连后拉取last_message_id之后的消息
- 离线消息: Redis/MySQL持久化，用户下次登录推送
- 心跳检测: Ping/Pong帧定期发送，检测死连接

### 设计模式类

**Q5: 单例模式的线程安全如何保证?**

- Meyer's Singleton: C++11保证static局部变量初始化线程安全
- Double-Checked Locking: 需要memory_order_acquire/release保证可见性
- mutex保护: 懒汉式需要加锁，性能差

**Q6: 如果连接池中的连接长时间未归还，如何处理?**

- 超时检测: 记录连接分配时间，定时扫描超时连接
- 强制回收: 超时连接强制放回池中
- 告警: 记录日志，定位泄漏代码
- 连接验证: 归还时mysql_ping验证连接有效性

**Q7: 为什么连接池使用std::list而不是std::vector?**

- 频繁删除: list删除O(1)，vector删除O(n)
- 不需要随机访问: 只需要front()取头部连接
- 迭代器稳定: list插入删除不影响其他迭代器

### MySQL相关

**Q8: MySQL连接为什么会断开?如何解决?**

- 原因: wait_timeout=28800秒(8小时)，网络闪断，MySQL重启
- 解决: mysql_ping检测并自动重连，连接池定期保活(SELECT 1)

**Q9: Prepared Statement如何防止SQL注入?**

- 参数化查询: 参数和SQL分离，参数不会被当作SQL执行
- mysql_stmt_bind_param: 参数绑定时指定类型，服务端验证
- 转义特殊字符: MySQL自动转义单引号等

**Q10: 数据库连接池最大连接数如何设置?**

- 公式: connections = (core_count × 2) + effective_spindle_count
- 考虑因素: CPU核心数、磁盘IO能力、MySQL max_connections限制
- 监控调优: 观察连接数使用率，动态调整

### Redis相关

**Q11: Redis为什么快?**

- 单线程: 避免上下文切换和锁竞争
- 内存操作: 纳秒级访问速度
- IO多路复用: epoll高效处理并发连接
- 高效数据结构: SDS，跳表，ziplist等优化

**Q12: 如何防止Redis缓存穿透/击穿/雪崩?**

- 穿透(查询不存在数据): 布隆过滤器拦截，缓存空对象
- 击穿(热点key过期): 互斥锁(setnx)，热点数据永不过期
- 雪崩(大量key同时过期): 过期时间加随机值，Redis Cluster

**Q13: Redis持久化RDB和AOF的区别?**

- RDB: 快照，适合备份，恢复快，可能丢失数据
- AOF: 追加日志，数据更完整，文件较大
- 生产环境: RDB + AOF混合持久化

### Kafka相关

**Q14: Kafka为什么快?高吞吐量如何实现?**

- 顺序写磁盘: 速度接近内存
- 零拷贝: sendfile系统调用，减少数据拷贝
- 批量发送: 消息打包发送，减少网络IO
- 分区并行: 多分区并行读写

**Q15: Kafka如何保证消息不丢失?**

- 生产者: acks=all，retries>0，幂等性
- Broker: replication.factor>=3，min.insync.replicas>=2
- 消费者: enable.auto.commit=false，手动提交offset

**Q16: Consumer Group的作用?**

- 同组内消费者共享分区，实现并行消费
- 每个分区只分配给组内一个消费者
- Rebalance: 消费者加入/退出时重新分配分区

### 网络编程类

**Q17: epoll的ET和LT模式区别?**

- LT(Level Triggered): 条件触发，有数据就通知，可以不一次性读完
- ET(Edge Triggered): 边缘触发，状态变化才通知，必须一次性读完
- Muduo使用LT: 编程简单，不易出错

**Q18: Reactor模式和Proactor模式的区别?**

- Reactor: 同步IO，应用程序负责read/write，Muduo采用
- Proactor: 异步IO，内核负责read/write，Windows IOCP

**Q19: 如何处理粘包/半包问题?**

- 固定长度: 每个消息固定N字节
- 分隔符: 消息以\r\n等结束
- 长度前缀: 前2/4字节表示长度
- WebSocket: 帧头包含payload length，Buffer缓冲不完整帧

**Q20: gRPC的连接管理?**

- Channel: gRPC连接抽象，内部维护连接池
- Subchannel: 实际TCP连接，Channel管理多个Subchannel
- 项目中: 每个CometClient一个Channel，失败时recreate

---

## 八、项目亮点总结

### 30秒电梯演讲

独立完成分布式实时消息推送系统，采用**三层微服务架构**，使用**Muduo网络库**处理WebSocket长连接，**gRPC**实现服务间通信，**Kafka**作为消息中间件，**Redis**缓存热点数据，**MySQL**持久化存储。解决了MySQL连接断开、Redis历史消息查询优化等技术难点，实现了完整的**Docker容器化部署**。

### 核心技术栈

```markdown
- 网络编程: Muduo (Reactor模式 + epoll + 多线程)
- RPC通信: gRPC + Protocol Buffers
- 消息队列: Kafka (高吞吐量 + 分区顺序)
- 缓存: Redis (List存储历史消息)
- 数据库: MySQL (连接池 + Prepared Statement)
- 部署: Docker + docker-compose
```

### 关键代码位置索引

```markdown
- 数据库连接池: logic/mysql/db_pool.cc:377-613
- Redis连接池: logic/redis/cache_pool.h
- Kafka生产者: logic/service/kafka_producer.h
- Kafka消费者: job/service/kafka_consumer.h
- WebSocket处理: chat-room/service/websocket_conn.h
- gRPC服务: chat-room/rpc/comet_service.h
- 房间订阅: chat-room/service/pub_sub_service.cc
- 消息存储: logic/service/message_service.h
```

---

## 附录

### A. 性能优化建议

1. **连接池优化**: 监控连接使用率，动态调整最大连接数
2. **Redis优化**: 使用Pipeline批量操作，减少网络RTT
3. **Kafka优化**: 增加分区数，提升并行度
4. **消息压缩**: Kafka启用压缩(snappy/lz4)，减少网络传输
5. **索引优化**: MySQL建立room_id、user_id、timestamp复合索引

### B. 监控指标

```
QPS: 每秒请求数
RT: 响应时间(P50/P95/P99)
错误率: 5xx错误占比
连接数: WebSocket并发连接数
消费lag: Kafka消费延迟
缓存命中率: Redis命中率
```

### C. 扩展方向

1. **消息审核**: 增加审核服务消费Kafka消息
2. **离线推送**: 支持APNs/FCM移动推送
3. **消息加密**: 端到端加密保护隐私
4. **限流降级**: Sentinel/Hystrix限流熔断
5. **分布式追踪**: Jaeger/Zipkin链路追踪



分布式如何实现

目前，一个chatroom对应一个logic服务器，一个logic服务器对应一个job服务器，一个job服务器对应一个comet服务器，如何实现不同层之间的多对多关系（目前配置文件写死了不同层的服务器地址）？

为何引入kafka，引入的优势是什么？

为何引入grpc，而不是直接由网关层发送广播的消息？
